import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import pandas as pd
import logging

# LangChain & vector store imports
from langchain.chains import RetrievalQA
from langchain_community.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain_groq import ChatGroq  # <- LLaMA via Groq
from config.config import logger, GROQ_API_KEY

# ---------- Chatbot Chain Setup ----------

def create_qa_chain(retriever) -> RetrievalQA:
    """
    Creates a RetrievalQA chain where LLaMA provides initial answers and ChatGPT supervises/refines them.
    """
    # Initialize memory for context tracking
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True, output_key="answer")
    logger.info(f"Initialized conversation memory.")

    # Step 1: LLaMA (via Groq) generates the initial response
    llama_llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name="llama3-8b-8192", temperature=0.7)
    logger.info("LLaMA initialized.")

    # Step 2: ChatGPT supervises/refines the response
    chatgpt_llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.5)
    logger.info("ChatGPT initialized.")

    # Shared prompt
    system_prompt = '''You are an intelligent assistant for Amazon sellers. 
        Your job is to analyze the product details (column by column), reviews, fetures and metadata to answer seller queries. 
        Your responses should be clear, concise, and Keep it short unless and untill the user mentioning about the detailedly of the product.
        Instructions:
        - If the user input is a greeting (e.g., "hi", "hello"), respond politely and ask them to specify a product-related question.
        - Avoid over-explaining or including raw review text unless explicitly asked.
        - Do not output markdown symbols (like ** or *) â€” just plain, readable text.
        - When no meaningful answer is possible, ask for clarification.
        - Format lists as clean paragraphs, not bullet points.

        Relevant Data:
        {context}

        Question: {question}
        '''
    PROMPT = PromptTemplate(template=system_prompt, input_variables=["context", "question"])

    # Step 1: Run the initial QA with LLaMA
    llama_qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llama_llm,
        retriever=retriever,
        # memory=memory,
        return_source_documents=True,
        combine_docs_chain_kwargs={'prompt': PROMPT}
    )
    llama_qa_chain.memory = memory
    logger.info("LLaMA QA chain ready.")

    # Step 2: Define a wrapper that feeds LLaMA's answer into ChatGPT
    class SupervisorChain:
        def __init__(self, inner_chain, supervisor_llm):
            self.inner_chain = inner_chain
            self.supervisor_llm = supervisor_llm
            # self.memory = inner_chain.memory  # reuse memory

        def invoke(self, inputs):
            llama_response = self.inner_chain.invoke(inputs)

            # Extract context (relevant documents)
            source_text = "\n\n".join([doc.page_content for doc in llama_response.get("source_documents", [])])
            question_text = inputs["question"] if isinstance(inputs, dict) else str(inputs)

            # Supervisor (ChatGPT) receives both the context and the LLaMA's draft response
            prompt = f"""You are a supervisor AI that improves answers generated by another assistant.

            Task:
                - Improve the draft answer for clarity and tone.
                - Do not include headings like "Question" or "Answer".
                - Remove markdown formatting (e.g., stars, backticks).
                - If the input is just a greeting like "hi", politely ask the seller to specify a product-related question.

            Input:
            Question: {question_text}


            Relevant Context:
            {source_text}

            Assistant's Draft Answer:
            {llama_response['answer']}
            """
            improved_response = self.supervisor_llm.invoke(prompt).content
            return {"answer": improved_response, "source_documents": llama_response.get("source_documents", [])}

        @property
        def memory(self):
            return self.inner_chain.memory

    qa_chain = SupervisorChain(llama_qa_chain, chatgpt_llm)
    logger.info("Supervisor QA chain ready.")
    return qa_chain
