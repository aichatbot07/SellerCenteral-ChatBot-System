import requests
import logging
import gzip
from google.cloud import storage
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Dataset URLs (replace with the correct Hugging Face URL)
DATASET_URLS = [
    "https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Health_and_Personal_Care.jsonl.gz",
    "https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/All_Beauty.jsonl.gz",
    "https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Musical_Instruments.jsonl.gz",
    "https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Appliances.jsonl.gz",
    "https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_2023/raw/review_categories/Amazon_Fashion.jsonl.gz"
]

BUCKET_NAME = "ai_chatbot_seller_central"

def create_retry_session():
    """Creates a session with retry logic."""
    session = requests.Session()
    retry = Retry(
        total=5,  # Total number of retries
        backoff_factor=1,  # Factor for delay between retries
        status_forcelist=[500, 502, 503, 504]  # Retry on server errors
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

def upload_to_gcs_from_url(url, bucket_name):
    """Download, extract in-memory, and upload directly to GCS."""
    filename = url.split("/")[-1]  # Extract file name (e.g., All_Beauty.jsonl.gz)
    jsonl_filename = filename.replace(".gz", "")  # Extracted JSONL filename

    logging.info(f"Downloading dataset: {filename}")

    session = create_retry_session()  # Use session with retries
    try:
        response = session.get(url, stream=True, timeout=300)  # Set timeout to 300 seconds for large files
        response.raise_for_status()  # Raises HTTPError for bad responses (4xx, 5xx)
    except requests.exceptions.RequestException as e:
        logging.error(f"Error downloading {filename}: {e}")
        return

    # Check if the content length matches the expected size
    content_length = response.headers.get('Content-Length')
    if content_length:
        logging.info(f"Content-Length for {filename}: {content_length} bytes")

    # Initialize GCS client and bucket
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(f"reviews/{jsonl_filename}")  # Store inside 'reviews/' folder in GCS

    try:
        # Read the gzip file as a stream and upload to GCS directly
        with gzip.GzipFile(fileobj=response.raw, mode="rb") as gzipped_file:
            blob.upload_from_file(gzipped_file, content_type="application/jsonl", rewind=True, size=1024 * 1024 * 50)  # 50MB chunks
        logging.info(f"Uploaded {jsonl_filename} to GCS successfully.")
    except Exception as e:
        logging.error(f"Error uploading {jsonl_filename} to GCS: {e}")

if __name__ == "__main__":
    for url in DATASET_URLS:
        upload_to_gcs_from_url(url, BUCKET_NAME)
